# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jh8Xmy-7VWoePSthlMd8suVQw_mTTPGO
"""

pip install pyclustering

"""# Mounting Google Drive Locally"""

from google.colab import drive
drive.mount("/content/drive/")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Machine Learning Assignments/Assignment3/2019224_HW3

"""# Import Modules"""

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import math

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from pyclustering.cluster.kmedians import kmedians
from sklearn.decomposition import PCA

"""# Import Dataset"""

df_population = pd.read_csv('datasets/population.csv')
df_more_than_50k = pd.read_csv('datasets/more_than_50k.csv')
df_data_description = pd.read_csv('datasets/Dataset Description.csv',index_col=0)

"""Classifying numerical and categorical columns using dataset descriptions"""

df_data_description.head()

categorical_cols = df_data_description[df_data_description.Type=='categorical'].index.tolist()
numerical_cols = df_data_description[df_data_description.Type=='numerical'].index.tolist()

"""# 1 Preprocessing

### 1.1 Replacing Missing Data : ? With NaN
"""

df_population.replace(' ?',np.NaN,inplace=True)
df_more_than_50k.replace(' ?',np.NaN,inplace=True)

"""### 1.2 Removing Columns Based on Percentage of Missing Values

Columns With Missing Values
"""

cols_with_missing_values_population = df_population.columns[df_population.isnull().any()]
cols_with_missing_values_50k = df_more_than_50k.columns[df_more_than_50k.isnull().any()]

"""Percentage of Missing Values for population dataset"""

df_population[cols_with_missing_values_population].isnull().sum() * 100 / len(df_population)

"""Percentage of missing values for more than 50k dataset"""

df_more_than_50k[cols_with_missing_values_50k].isnull().sum()*100/len(df_more_than_50k)

"""Columns more than 40% missing values and are dropped"""

cols_to_drop = ['MIGMTR1','MIGMTR3','MIGMTR4','MIGSUN']
df_population.drop(columns=cols_to_drop,inplace=True)
df_more_than_50k.drop(columns=cols_to_drop,inplace=True)
for col in cols_to_drop:
  categorical_cols.remove(col)

"""# 2 Feature Analysis

### 2.1 Histograms of Numerical Features
"""

def histogram_numerical(data):
  data[numerical_cols].hist(figsize=(10,10),bins=10)
  plt.tight_layout()
  plt.show()

"""For population dataset - The columns AHRSPAY, CAPGAIN, CAPLOSS, DIVVAL have more than 70% values in their one of the columns. Such columns should be dropped."""

histogram_numerical(df_population)

"""For more than 50k dataset - All columns here except the AAGE column have more than 70% of their values in one of the columns. However, since the operations performed on this dataset should align with the those performed on population dataset, the same columns are dropped. """

histogram_numerical(df_more_than_50k)

"""### 2.1 Histograms of Categorical Features"""

def histogram_categorical(data):
  """
  For categorical features value counts are used to plot the histogram.
  """
  for idx,col in enumerate(categorical_cols):
    plt.subplot(12,3,idx+1)
    data[col].value_counts().plot.bar(figsize=(20,100))
    plt.title(col)

  plt.suptitle("Histograms of Categorical Features")
  plt.tight_layout()
  plt.show()

"""For population dataset - The categorical columns - ['AHSCOL',
 'ARACE',
 'AREORGN',
 'AUNMEM',
 'AUNTYPE',
 'GRINREG',
 'GRINST',
 'PARENT',
 'PEFNTVTY',
 'PEMNTVTY',
 'PENATVTY',
 'PRCITSHP',
 'SEOTR',
 'VETQVA',
 'VETYN'] have more than 70% values in one the columns. Such columns are dropped.
"""

histogram_categorical(df_population)

"""For population more than 50k dataset - the columns ['AHSCOL',
 'AMARITL',
 'ARACE',
 'AREORGN',
 'ASEX',
 'AUNMEM',
 'AUNTYPE',
 'FILESTAT',
 'GRINREG',
 'GRINST',
 'HHDREL',
 'PARENT',
 'PEFNTVTY',
 'PEMNTVTY',
 'PENATVTY',
 'PRCITSHP',
 'SEOTR',
 'VETQVA',
 'VETYN'] have more than 70% values in one on the columns. Since the operations performed on this dataset should align with the population dataset, the same columns will be dropped.
"""

histogram_categorical(df_more_than_50k)

"""### 2.2 Dropping Features"""

threshold = 70

"""From the Histograms a threshold value of 70% was decided. Columns with >= 70% values in one column were dropped"""

print("Percentage of values in majority column (Categorical):")

cols_to_drop_numerical = []

for col in numerical_cols:
  df_cut = pd.cut(df_population[col],10)
  percentage = df_cut.value_counts().max()/len(df_cut)*100
  print(col,math.trunc(percentage),"%")
  if percentage>=threshold:
    cols_to_drop_numerical.append(col)

print("Categorical columns dropped")
cols_to_drop_numerical

print("Percentage of values in majority column (Numerical):")

cols_to_drop_categorical = []

for col in categorical_cols:
  percentage = df_population[col].value_counts().max()/len(df_population)*100
  print(col,math.trunc(percentage),"%")
  if percentage>=threshold:
    cols_to_drop_categorical.append(col)

print("Numerical columns dropped")
cols_to_drop_categorical

#removing the dropped columns from the columns lists and dropping columns

for col in cols_to_drop_numerical:
  numerical_cols.remove(col)

for col in cols_to_drop_categorical:
  categorical_cols.remove(col)

cols_to_drop = cols_to_drop_numerical+cols_to_drop_categorical

# same colummns were dropped from the 2 datasets since the operations should align
df_population.drop(columns=cols_to_drop,inplace=True)
df_more_than_50k.drop(columns=cols_to_drop,inplace=True)

"""# 3 Imputation, Bucketization, One Hot Encoding

### 3.1 Replace Missing Values With Mode
"""

cols_with_missing_values = df_population.columns[df_population.isnull().any()]

modes = {}
for col in cols_with_missing_values:
  modes[col] = df_population[col].mode()[0]
  df_population[col].fillna(modes[col],inplace=True)
  df_more_than_50k[col].fillna(modes[col],inplace=True)

"""### 3.2 Bucketize Numerical Features"""

for col in numerical_cols:
  df_population[col],bins = pd.cut(df_population[col],10,retbins=True)
  df_more_than_50k[col] = pd.cut(df_more_than_50k[col],bins) #same bins used as population dataset

"""### 3.3 One Hot Encode"""

encoder = OneHotEncoder()
arr_population = encoder.fit_transform(df_population).toarray()
arr_50k = encoder.transform(df_more_than_50k).toarray()

df = pd.DataFrame(arr_population)
df_50k = pd.DataFrame(arr_50k)

df.head()

df_50k.head()

"""### 3.4 PCA on Population Dataset

A) Cumulative Variance vs Number of Components
"""

pca = PCA(n_components = 100, random_state=0)
df_pca = pca.fit_transform(df)
print("The cumulative variance percentage is ",np.cumsum(pca.explained_variance_ratio_)*100," for 100 PC")

#plotting cumulative variance vs number of components

fig, ax = plt.subplots()
x = np.arange(pca.n_components_) + 1
y = np.cumsum(pca.explained_variance_ratio_)*100
plt.plot(x,y)
plt.axhline(y=80, color='r', linestyle='--')
plt.xticks(np.arange(0, 100, step=5))
plt.xlabel("Number of Components")
plt.ylabel("Cummalative Variance")
ax.grid(axis='x')
plt.show()

"""B) Value 32 was chosen by taking the threshold as above 80

C) Fitting PCA again with chosen value
"""

pca = PCA(n_components=32,random_state=0)
df_pca = pca.fit_transform(df)
df = pd.DataFrame(df_pca)

"""### 3.5 PCA On 50K Dataset"""

df_50k = pd.DataFrame(pca.transform(df_50k))

df.head()

df_50k.head()

"""# 4 Clustering on population dataset

### 4.1 K-Median Clustering With Varying Values of K
"""

def kmedians_clustering_varying_k_values(data,folder):
  """
  apply k-medians clustering for a range of k values and plot graphs for average within cluster distance
  and number of clusters
  """
  distance_cluster = []
  distance_point = []

  print("Average Within Cluster Distances\n")

  for k in range(1,25):
    filename = folder+'/clusters k='+str(k)+'.pickle'
    if os.path.exists(filename):

      with open(filename,'rb') as file:
        kmedians_ = pickle.load(file)

    else:

      #chosing initial medians as random k rows
      arr = np.random.choice(len(data),k,replace=False)
      initial_medians = data.loc[arr]

      #fit kmedians
      kmedians_ = kmedians(data,initial_medians)
      kmedians_.process()

      with open(filename,'wb+') as file:
        pickle.dump(kmedians_,file)

    #get the distance
    dist_within_a_cluster = np.around(kmedians_.get_total_wce()/k,decimals=2)
    dist_for_a_point = np.around(kmedians_.get_total_wce()/len(data),decimals=2)
    print("k =",k,", for a cluster",dist_within_a_cluster," for a point",dist_for_a_point)
    distance_cluster.append(dist_within_a_cluster)
    distance_point.append(dist_for_a_point)

  print()
  plt.plot(np.arange(1,25),distance_cluster)
  plt.xlabel("Number of Clusters")
  plt.ylabel("Average Within Cluster Distance for a cluster")
  plt.show()

  print()
  plt.plot(np.arange(1,25),distance_point)
  plt.xlabel("Number of Clusters")
  plt.ylabel("Average Within Cluster Distance for a point")
  plt.show()

kmedians_clustering_varying_k_values(df,'Weights/clusters')

"""### 4.2

The value chosen for k is 23 as the graph starts becoming constant around this value and the elbow is observed. The value 4 could also be chosen from the elbow curve but it does not lie in the provided range [10,24].
"""

chosen_k = 13

"""###4.3 K-Median Clustering With Best Value Chosen"""

with open('Weights/clusters/clusters k='+str(chosen_k)+'.pickle','rb') as file:
  kmedians_best = pickle.load(file)

"""# Approach 1: Clustering using Population Dataset Clusters

## 5 Clustering on more than 50k Dataset
"""

def euclidean_distance(point_start, point_end):
  """
  calculates and returns the euclidean distance between two points start and end
  """
  dist = 0
  for i in range(len(point_start)):
    dist = dist + math.pow(point_start[i]-point_end[i],2)
  return math.pow(dist,0.5)

def wce(data,y_pred,model):
  """
  returns the sum of euclidean distances of each point to its closest cluster median
  """
  medians = model.get_medians()
  wce_ = 0
  for index,row in data.iterrows():
    label = y_pred[index]
    wce_+=euclidean_distance(row,medians[label])
  return wce_

"""### Varying Values of K"""

distance_cluster = []
distance_point = []

print("Average Within Cluster Distances\n")

for k in range(1,25):
  with open('Weights/clusters/clusters k='+str(k)+'.pickle','rb') as file:
    kmedians_ = pickle.load(file)
  
  #predict clusters
  y_pred = kmedians_.predict(df_50k.values.tolist())

  #calculate the distance
  dist = wce(df_50k,y_pred,kmedians_)

  dist_within_a_cluster = np.around(dist/k,decimals=2)
  dist_for_a_point = np.around(dist/len(df_50k),decimals=2)
  print("k =",k,", for a cluster",dist_within_a_cluster," for a point",dist_for_a_point)
  distance_cluster.append(dist_within_a_cluster)
  distance_point.append(dist_for_a_point)

#plot graphs
print()
plt.plot(np.arange(1,25),distance_cluster)
plt.xlabel("Number of Clusters")
plt.ylabel("Average Within Cluster Distance for a cluster")
plt.show()

print()
plt.plot(np.arange(1,25),distance_point)
plt.xlabel("Number of Clusters")
plt.ylabel("Average Within Cluster Distance for a point")
plt.show()

"""### Clustering with the best chosen value from part 4"""

y = kmedians_best.predict(df_50k.values.tolist())

"""##6 Comparing More Than 50k With Population Data

### 6.1 Comparing Proportion of Data in each cluster for more than 50k data to the proportion of data in each cluster for general population
"""

def compare_proportions(len_of_clusters_population,len_of_clusters_50k):
  """
  takes the lengths of the clusters as arguments and returns the proportions
  """

  print("Cluster Lengths for Population Dataset",len_of_clusters_population)
  print("Cluster Length for More Than 50k Dataset",len_of_clusters_50k)

  len_50k = len(df_50k)
  len_population = len(df)

  proportion_population = np.array([len/len_population for len in len_of_clusters_population])
  proportion_50k = np.array([len/len_50k for len in len_of_clusters_50k])

  print("\nProportion of data in each cluster for population dataset",proportion_population)
  print("Proportion of data in each cluster for More than 50k dataset",proportion_50k)

  return proportion_population,proportion_50k

len_of_clusters_population = np.array([len(cluster) for cluster in kmedians_best.get_clusters()])
len_of_clusters_50k =  np.bincount(y)

prop_pop, prop_50k = compare_proportions(len_of_clusters_population,len_of_clusters_50k)

"""### 6.2 Over-represented Clusters"""

def overrepresented_clusters(dataset_main_prop,dataset_sec_prop):
  """
  takes 2 lists of proportions,, and for each index calculates the representation percentages
  for each cluster index using the formula differences_of_proportions*100.

  A value greater than 0 denotes that the cluster is overrerpresented as compared to the second dataset
  """

  for i in range(len(dataset_main_prop)):
    rep_percentage = (dataset_main_prop[i] - dataset_sec_prop[i])*100
    if(rep_percentage>0):
      print("Cluster",i," is overrepresented by",rep_percentage,"%")

"""Clusters overrepresented in population data as compared to more than 50k data"""

overrepresented_clusters(prop_pop,prop_50k)

"""Clusters overrepresented in more than 50k population data as compared to population data"""

overrepresented_clusters(prop_50k,prop_pop)

"""### 6.3 and 6.4"""

# datasets before applying pca

df_pop_before_pca = pd.DataFrame(arr_population,columns = encoder.get_feature_names())
df_50k_before_pca = pd.DataFrame(arr_50k,columns = encoder.get_feature_names())

def people_in_overrepresented_clusters(cluster_index,model):
  """
  returns the kinds of people that are a part of a cluster that is overrepresented
  """

  #get the median of the cluster index using the model from the arguments
  median = model.get_medians()[cluster_index]
  #inverse transform the median using pca to map to orignal features
  inverse_transformed_median = pca.inverse_transform(median)
  #round the values to 0 and 1 as was there in the orignal one hot encoded array
  inverse_transformed_median = [round(a) for a in inverse_transformed_median]

  #using values of first principal component to sort the features 
  feature_select = pd.DataFrame(pca.components_,columns=df_50k_before_pca.columns).T[0]
  #get feature names after one hot encoding
  col_names = encoder.get_feature_names()
  #get actual feature names using dataset description
  actual_feature_names = df_data_description.loc[df_population.columns]['Column']

  #stores the feature name, its value as key and PC1 value as the value
  features = {}

  for i in range(len(inverse_transformed_median)):
    if inverse_transformed_median[i]==1:
      index = int(col_names[i][1:col_names[i].index("_")])
      feature_name = actual_feature_names.iloc[index]
      features[feature_name+": "+col_names[i]] = feature_select[col_names[i]]

  #sort by PC1 values in descending order
  return pd.DataFrame.from_dict(features,orient='index').sort_values(ascending=False,by=0).head(3)

"""**For cluster 4 -overrepresented in 50k dataset**

This cluster represents people who have worked for 46.8-52 weeks in a year, have tax filer status as joint both under 65 and married civilians wiht spouse.
"""

people_in_overrepresented_clusters(4,kmedians_best)

"""**For cluster 8 overrepresented in population dataset**

This cluster represents males, with data collected in year 1995 and not in the universe under 1 year old.
"""

people_in_overrepresented_clusters(8,kmedians_best)

"""# Approach 2: Clustering Seperately

## 5 Clustering on more than 50k Dataset

### Varying Values of K
"""

kmedians_clustering_varying_k_values(df_50k,'Weights/clusters_50k')

"""### Clustering with the best chosen value from part 4 """

with open('Weights/clusters_50k/clusters k='+str(chosen_k)+'.pickle','rb') as file:
  kmedians_50k_best = pickle.load(file)

"""## 6 Comparing More Than 50K With Population Data

### 6.1 Comparing Proportion of Data in each cluster for more than 50k data to the proportion of data in each cluster for general population
"""

len_of_clusters_50k = np.array([len(cluster) for cluster in kmedians_50k_best.get_clusters()])

prop_pop, prop_50k = compare_proportions(len_of_clusters_population,len_of_clusters_50k)

"""### 6.2 Over-represented Clusters

Clusters overrepresented in population data as compared to more than 50k data
"""

overrepresented_clusters(prop_pop,prop_50k)

"""Clusters overrepresented in more than 50k population data as compared to population data"""

overrepresented_clusters(prop_50k,prop_pop)

"""### 6.3 and 6.4

**For cluster 11 overrepresented in 50k**

This cluster repersents people who have worked 46.8 to 52 weeks in a year, with class of worker as private and tax filer status as joint both under 65.
"""

people_in_overrepresented_clusters(11,kmedians_50k_best)

"""**For cluster 8 overrepresented in population dataset**

This cluster represents males, with data collected in the year 1995 and not in the universe under 1 year old.
"""

people_in_overrepresented_clusters(8,kmedians_best)

