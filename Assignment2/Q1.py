# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_L16dEVii7LdAaaZgCFV_VvLuhbBdEHO

# Importing Modules
"""

import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
import os
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier

"""# Importing Dataset"""

df = pd.read_csv('data.csv')
df.drop(columns=['No'],inplace=True)
df.head()

"""# Dropping Missing Value Rows"""

df.dropna(inplace=True)

"""# Encoding Categorical Columns"""

cols = ['year','cbwd']
label_encoder = LabelEncoder() 
df[cols] = df[cols].apply(label_encoder.fit_transform)

y = df['month']
X = df.drop(columns=['month'])

"""# Splitting Dataset Into Train, Validation and Test Sets"""

def train_val_test_split(data,random_state,val_size=0.15,test_size=0.15):
  """
  Train Valdiation Test split implemented from scratch. Takes the dataset,
  val_size, test_size and random_state as parameters and returns the train,
  validation and test sets.

  Calculates the length of train, test and validation sets, shuffles the 
  dataset and returns the 3 sets.
  """
  
  shuffled_data = data.sample(frac=1,random_state=random_state)

  val_size = int(len(data)*val_size)
  test_size = int(len(data)*test_size)
  train_size = len(data)-val_size-test_size

  train = shuffled_data[:train_size]
  val = shuffled_data[train_size:train_size+val_size]
  test = shuffled_data[train_size+val_size:]

  return train,val,test

df_train, df_val, df_test = train_val_test_split(df,random_state=42)

"""# Splitting Into X and y values"""

y_train = df_train['month']
X_train = df_train.drop(columns=['month'])

y_val = df_val['month']
X_val = df_val.drop(columns=['month'])

y_test = df_test['month']
X_test = df_test.drop(columns=['month'])

def accuracy(y_true,y_pred):
  y_true = y_true.to_numpy()
  return np.sum(y_true==y_pred)/len(y_true)

"""# A. Training Decision Tree Model"""

def train_predict_evaluate(model):
  model.fit(X_train,y_train)
  y_pred = model.predict(X_test)
  print("Accuracy: ",accuracy(y_test,y_pred))

train_predict_evaluate(DecisionTreeClassifier(criterion='gini',random_state=0))

train_predict_evaluate(DecisionTreeClassifier(criterion='entropy',random_state=0))

"""# B. Training Decision Tree Model on Different Max Depths"""

depths = [2,4,8,10,15,30]
train_accuracy = []
test_accuracy = []

for depth in depths:
  decision_tree = DecisionTreeClassifier(criterion='entropy',max_depth=depth,random_state=0)
  decision_tree.fit(X_train,y_train)
  y_pred_train = decision_tree.predict(X_train)
  y_pred_test = decision_tree.predict(X_test)
  train_accuracy.append(accuracy(y_train,y_pred_train))
  test_accuracy.append(accuracy(y_test,y_pred_test))

print("Train Accuracy: ",train_accuracy)
print("Test Accuracy: ",test_accuracy)

plt.plot(depths,train_accuracy,label='train accuracy')
plt.plot(depths,test_accuracy,label='test accuracy')
plt.xlabel("Depth")
plt.ylabel("Accuracy")
plt.title("Accuracy vs Depth")
plt.legend()
plt.show()

"""# C."""

def random_selection(data,size=0.5):
  """
  Trains the training data and returns a random 50%.
  """

  shuffled_data = data.sample(frac=0.5)
  return shuffled_data

def ensemble(n_estimators=100,max_depth=3):
  """
  Combines 100 decision tree stumps trained on random 50% of the training data, predicts the
  testing samples by taking mode over the predictions.
  """
  predictions = []

  for i in range(n_estimators):
    data = random_selection(df_train)
    y = data['month']
    X = data.drop(columns=['month'])
    decision_tree = DecisionTreeClassifier(criterion='entropy',max_depth=max_depth,random_state=0)
    decision_tree.fit(X,y)
    predictions.append(decision_tree.predict(X_test))

  y_pred = pd.DataFrame(predictions).T.mode(axis='columns')[0]
  y_test_ = y_test.reset_index(drop=True)

  print("Accuracy: ",accuracy(y_test_,y_pred))

ensemble()

"""# D."""

def tuning(n_estimators=100):
  """
  Function for tuning decision stump by changing max depth and n estimators.
  Train, Validation and Test Accuracies are returned for different values
  of max_depth and n_estimators
  """
  depths = [4,8,10,15,20,30]
  results = []
  y_test_ = y_test.reset_index(drop=True)

  #trains decision tree stumps on different depths, stores the train, test and
  # validation predictions

  for depth in depths:
    predictions_train = []
    predictions_val = []
    predictions_test = []
    for i in range(n_estimators):
      data = random_selection(df_train)
      y = data['month']
      X = data.drop(columns=['month'])
      decision_tree = DecisionTreeClassifier(criterion='entropy',max_depth=depth,random_state=0)
      decision_tree.fit(X,y)
      predictions_train.append(decision_tree.predict(X_train))
      predictions_val.append(decision_tree.predict(X_val))
      predictions_test.append(decision_tree.predict(X_test))

    # takes mode over predictions to find y_pred
    num = [4,8,10,15,20,100]
    for n in num:
      y_pred_train = pd.DataFrame(predictions_train[:n]).T.mode(axis='columns')[0]
      y_pred_val = pd.DataFrame(predictions_val[:n]).T.mode(axis='columns')[0]
      y_pred_test = pd.DataFrame(predictions_test[:n]).T.mode(axis='columns')[0]
      accuracy_train = accuracy(y_train,y_pred_train)
      accuracy_val = accuracy(y_val,y_pred_val)
      accuracy_test = accuracy(y_test,y_pred_test)
      result = {'depth':depth,
                      'n_estimator':n,
                      'train accuracy':accuracy_train,
                      'validation accuracy': accuracy_val,
                      'test accuracy': accuracy_test}
      print(result)
      results.append(result)

  return results

def save_and_load_results():
  if not os.path.exists("D_results.csv"):
    results = tuning()
    res = pd.DataFrame.from_dict(results)
    res.to_csv('D_results.csv')
  return pd.read_csv('D_results.csv',index_col=0)

D_results = save_and_load_results()

print(display(D_results))

"""# E.

#### Adaboost with Decision Tree as base estimator and different n_estimators
"""

number_of_estimators = [4,8,10,15,20]

for num in number_of_estimators:
  ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',random_state=0),n_estimators=num)
  ada_boost.fit(X_train,y_train)
  y_pred = ada_boost.predict(X_test)
  print("Accuracy Score for n_estimators = "+str(num)+": "+str(accuracy(y_test,y_pred)))

